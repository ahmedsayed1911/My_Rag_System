import streamlit as st
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# --- Custom Stuff Chain (Ø¨Ø¯ÙŠÙ„ create_stuff_documents_chain Ùˆ LLMChain) ---
def create_stuff_documents_chain(llm, prompt):
    """
    Custom simplified version of create_stuff_documents_chain
    that works even if LLMChain is unavailable.
    """
    class SimpleDocumentChain:
        def __init__(self, llm, prompt):
            self.llm = llm
            self.prompt = prompt

        def combine_docs(self, docs, input=None):
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø³ÙŠØ§Ù‚ ÙˆØ§Ø­Ø¯
            context = "\n\n".join([doc.page_content for doc in docs])
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø±Ø³Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
            prompt_text = self.prompt.format(context=context, input=input)
            # ØªÙ…Ø±ÙŠØ± Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø©
            response = self.llm.invoke(prompt_text)
            return response.content if hasattr(response, "content") else str(response)

    return SimpleDocumentChain(llm, prompt)

# --- Custom Retrieval Chain ---
def create_retrieval_chain(retriever, document_chain):
    class SimpleRetrievalChain:
        def __init__(self, retriever, document_chain):
            self.retriever = retriever
            self.document_chain = document_chain

        def invoke(self, inputs):
            query = inputs.get("input", "")
            retrieved_docs = self.retriever.get_relevant_documents(query)
            answer = self.document_chain.combine_docs(retrieved_docs, input=query)
            return {
                "answer": answer,
                "context": retrieved_docs
            }

    return SimpleRetrievalChain(retriever, document_chain)


# --- Streamlit UI setup ---
st.set_page_config(page_title="My RAG System", page_icon="ğŸ¤–")
st.title("ğŸ“š My RAG System â€” Ask Your PDF")

# --- API Key ---
api_key = st.secrets.get("GROQ_API_KEY")
if not api_key:
    st.error("Please set your GROQ_API_KEY in Streamlit Secrets.")
    st.stop()

# --- Upload PDF ---
uploaded_file = st.file_uploader("ğŸ“„ Upload your PDF file", type="pdf")

if uploaded_file:
    with st.spinner("Processing your PDF..."):
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„Ù‚Ø±Ø§Ø¡ØªÙ‡ Ø¨ÙˆØ§Ø³Ø·Ø© PyPDFLoader
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_path = tmp_file.name

        # ØªØ­Ù…ÙŠÙ„ ÙˆÙ‚Ø±Ø§Ø¡Ø© PDF
        loader = PyPDFLoader(tmp_path)
        docs = loader.load()

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ chunks
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        splits = splitter.split_documents(docs)

        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªØ¬Ù‡ÙŠØ©
        embedding = FastEmbedEmbeddings()
        vectorstore = Chroma.from_documents(splits, embedding=embedding)
        retriever = vectorstore.as_retriever()

        # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Groq (LLaMA 3.3)
        llm = ChatOpenAI(
            model="llama-3.3-70b-versatile",
            openai_api_base="https://api.groq.com/openai/v1",
            openai_api_key=api_key,
            temperature=0.5,
            max_tokens=512
        )

        # Ø¥Ø¹Ø¯Ø§Ø¯ prompt
        prompt = PromptTemplate.from_template("""
        Use the following context to answer the question.
        If you don't know the answer, just say "I don't know."

        Context:
        {context}

        Question: {input}
        """)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„ÙŠØ¯ÙˆÙŠØ©
        document_chain = create_stuff_documents_chain(llm, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        st.success("âœ… PDF processed successfully!")

        # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¬ÙˆØ§Ø¨
        question = st.text_input("ğŸ’¬ Ask a question about your PDF:")
        if st.button("Get Answer") and question:
            with st.spinner("ğŸ¤” Thinking..."):
                response = retrieval_chain.invoke({"input": question})
                st.write("### ğŸ¤– Answer:")
                st.write(response["answer"])

                # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø¥Ù† ÙˆØ¬Ø¯Øª)
                sources = response.get("context", None)
                if sources:
                    st.write("### ğŸ“š Sources used:")
                    for i, doc in enumerate(sources):
                        st.markdown(f"**Chunk {i+1}:** {doc.page_content[:300]}...")

else:
    st.info("â¬†ï¸ Please upload a PDF file to begin.")
